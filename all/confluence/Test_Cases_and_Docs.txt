title: Test Cases and Docs
body: ## Test Cases

Live Document :
<https://docs.google.com/spreadsheets/d/1obzrOhwGBR9grXlrPMLVmzH_36XGeKG0VuJgZJMwRuY/>  
  

  

## Tools Used

Tools that we have at our disposal

 **1\. Keyword Driven -**  Keyword driven is an in house built application
which helps us to automate test cases. The process is quite simple and easy to
use. The application is built on selenium and it uses an specific excel file
to get the data required to perform automation test cases as per the
requirements line by line. One has to edit that excel file once a new feature
comes in and write down the necessary locators in the script based on the
business workflow. The application will execute each line individually one
after another and complete the test cases.

 **2\. ARYA**  \- ARYA is tool that we have developed to perform API endpoint
testings. Previously we had to click each API endpoint manually one by one to
check the statuses. Since clicking on every API endpoint is a time consuming
and hectic task we started working on to build this application ( Based on
Spring MVC). We have automated the whole process of of API testing by
developing ARYA. Whenever a new API is added to our system, we simply go to a
json file and add the API url and other parameters as a JSON object. Running
the application would iterate through the JSON file and parse the JSON objects
and generate URL for specific API endpoint and hit that URL. Whenever we need
to test the API or perform a regression test, we just run the application and
do other tasks. After the completion we can just take a look at the log and
get an idea which of these API needs to have a concern.

 **3\. JMETER** \- JMETER is an open source tool widely used for simulated
Load and Performance Testing. For InfoREACH, we use JMETER to use both
performance and soak testing. If anyone gets cucrious why I haven't mentioned
about load testing here i have got answers for you. Since JMETER runs on our
local machine, it creates simulated users as per our resources and often fails
to provide accurate results for a small portion. We can't put enough load to
the system using JMETER using our machines for this specific reason. So how do
we do performance and stress tesing? Load testing usually focuses on the
changes of behaviour of our application under certain load pressure where as
both soak/stress/ performance (Definition is given below) test relates to the
time constraint abruptly.

 **4\. Blazemeter-**

i. We use Blazemeter ( A freemium web based tool based on JMETER) for load
testing. We use load test of any feature starting with 10 concurrent user up
to 50~100. If we are happy with the result ( See Benchmarking), we go perform
both stress and performance test using JMETER.  
ii. Blazemeter provides a detailed report with an option to get KPI based
reports and it is very helpful for us to get an idea how the engine health is
and what are the numbers actually meaning.( avg respons time, throughput etc.)  
iii. It generates Loads from cloud machines.  
iv.Multiple scenarios can be test at once.  
v. We can choose different geo located servers to put the load on.

 **5\. Apache Benchmarking Tool** \- It is used mainly to test the initial
stability of any feature by providing specific number of hits/sec data and
other specific parameters. It will show the simulated user data for processing
both min and max request time.  
  

  

## Test Types

 **Functionality Testing-**  This is used to check if the product is as per
the specifications we intended for it as well as the functional requirements
the we charted out for it in our developmental documentation.  
Web based Testing Activities includes:

1\. Test all links in our webpages are working correctly and make sure there
are no broken links. Links to be checked will include -  
i. Outgoing links  
ii. Internal links  
iii. Anchor Links  
iv. MailTo Links

2\. Test Forms are working as expected. This will include-  
i. Scripting checks on the form are working as expected. For example- if a
user does not fill a mandatory field in a form an error message is shown.  
ii. Check default values are being populated  
iii.Once submitted, the data in the forms is submitted to a live database or
is linked to a working email address  
iv. Forms are optimally formatted for better readability

3\. Test Cookies are working as expected or not. Cookies are small files used
by websites to primarily remember active user sessions so someone doesn't need
to log in every time while visiting our application. Cookie Testing includes-  
i. Testing cookies (sessions) are deleted either when cache is cleared or when
they reach their expiry.  
ii. Delete cookies (sessions) and test that login credentials are asked for
when you next visit the site.

4\. Test business workflow- This includes  
i. Testing end - to - end workflow/ business scenarios which takes the user
through a series of modules to complete (i.e. Quick Campaign).  
ii. Test negative scenarios as well, such that when a user executes an
unexpected step, appropriate error message or help is shown in our
application.

 **Compatibility testing:**  
Compatibility tests ensures that InfoREACH displays correctly across different
devices. This would include-

Browser Compatibility Test: Same application in different browsers will
display differently. This makes sure that, InfoREACH is being displayed
correctly across browsers, JavaScript, AJAX and authentication is working
fine. WE haven't tested it thoroughly for mobile comaptibility. We are
planning to do it in alpha staging.

OS Compatibility: The rendering of web elements like buttons, text fields etc.
changes with change in Operating System. WE have tested in Windows, Linux and
MacOS for Os compatibility testing.

 **Performance Testing :**  This measures the response time of an application
with an expected number of users. The aim of this is to get a baseline and an
indication of how an application behaves under normal conditions. Does it meet
the required response time or not.

It is all about response time, request time, throughput time and mean time of
application.

 **Load Testing :**  This measures the response time when the application is
subjected to more than usual load. The application will be slower under heavy
load, but the aim of load testing is to see whether the application can
sustain the increased load on the server or will it crash and kill the
servers. Load is more about characterizing / simulating your actual workload.
Load testing is usually started as low numbers and gradually increased over a
given period of time until it reaches the desired load on the system and then
it ramps down.

It is all about testing behavior under normal and peak workload conditions.

 **Stress Testing :**  Stress Testing is like load testing but we keep on
increasing the load on the server till the time it crash down. The aim of
stress testing is to test the insane limits of an application. Stress testing
starts of the same as load testing, e.g. gradually increasing load on the
servers, the process is repeated till the time the load is reached to the
unexpected limits.

It is all about surfacing issues under extreme conditions and resource
failures.

 **User Acceptance Tesing-**  User Acceptance is defined as a type of testing
performed by the Client to certify the system with respect to the requirements
that was agreed upon. ... In VModel, User acceptance testing corresponds to
the requirement phase of the Software Development life cycle(SDLC).

Regression Testing: After any kind of major feature integration or overhauling
of a specific features a regression testing is performed. It consists all of
the above mentioned testings and we produce a report. I show the report to
Nahid in case of any kind of due deployment for the pilot server. If the
Regression testing has passed, I let Nahid know and the feature or changes are
deployed in the pilot server.

Soak Testing- After any kind of major business flow changes we perform a soak
testing. Soak testing usually is performed for longer hours to test the
consistency of our application for longer hours. Usually, if we have any kind
of major business flow changes, I turn up both Jmeter and Blazemeter and leave
them running. I usually do this during weekends. The application faces a heavy
user engagements for 24-72 hours and any kind of inconsistency, we get a log
report and find flaws of the application.

  

## BenchMarking

For benchmarking we check the average response time per user interaction with
our application. these are the metrics for user engagement responses per page-  
1\. Best (~0.1 seconds)  
2\. Good (0.1~1 seconds)  
3\. Ok (1~6 Seconds)

Anything above 6 seconds we count it as unresponsive and block if there is any
due deployment. For performance testing of Soak testing with 500 concurrent
Users, We consider (1~6 seconds) as a good benchmark.

For Amount of users, these are the metrics for user engagement per page-  
1\. Solo (One user accessing one page at a time)  
2\. Group ( 10 users accessing one page at different times/ 10 users accessing
specific one page at a specific time)  
3\. Soak (50~ 100 users accessing one page at a specific time/ 500 users
accessing one page at random time )

Assumptions for load testing on my local machine ( 8 gb Ram, 2 core Processor)

Simulate average traffic scenario - 10 requests / second test  
Simulated Users: 10 threads  
Ramp Period: 1 sec  
Duration: 30 min

Simulate medium traffic scenario - 30 requests / second test  
Simulated Users: 30 threads  
Ramp Period: 1 sec  
Duration: 30 min

Simulate super high traffic scenario - 100 requests / second test  
Simulated Users: 100 threads  
Ramp Period: 1 sec  
Duration: 30 min

Simulate an attack scenario - 350 requests / sec (based on max connection of
500)  
Simulated Users: 100 threads  
Ramp Period: 1 sec  
Duration: 10 min  
  
While running these test, I plan to monitor the following:  
CPU Load average  
RAM Load average  
Average Response time  
  

  

## Process Flow

There are different types of process flows that we maintain to ensure the
integrity of the application.

 **Simple Test Flow Chart-**

  

 **Testing a new feature**

1\. Gather requirements.  
2\. Set boundary cases for that features and write testcases amd make an entry
in the easyredmine as scrum task to do.  
3\. If it's possible to automate that flow, using keyword driven a automation
flow script is written.  
4\. Add the API endpoints to Arya as a JSON object.  
5\. Run automation test.  
6\. Run API testing.  
7\. If both works perfectly the flow will be recorded using Blazemeter as a
Jmx file.  
8\. Run the JMX file for 10~50 users under different usecases.  
9\. If the Blazemeter test results are satisfactory then we do performance,
load and stress testing using Jmeter and A/B testing tool.  
10\. After completing all the steps a regression test is performed to check
whetther the recent changes has made any impact on the current application.  
11\. Based on the reports, if the outcome for each test are satisfactory then
the feature is declared as Scrum done and Nahid deploys it to the Pilot
server.

 **Retesting a feature (After bug fixing)**

1\. Make an entry in the easyredmine as scrum task to do.  
2\. Test the flow for a happy path scenario using UI automation framework  
3\. Check the API endpoints of that flow  
4\. If the happy path works for a single user we add more loads continuously  
5\. 10 users at a time and up to fifty  
6\. Then a functional tesing is performed  
7\. Based on the reports, if the outcome for each test are satisfactory then
the ticket is declared closed and Nahid deploys it to the Pilot server.

 **Regression Testing**

1\. Make an entry in the easyredmine in ticket
[#7555](https://i3dev.easyredmine.com/issues/7555 "Scrum-Task: Regression
Testing \(Scrum-InProgress\)").  
2\. Test the flow for a happy path scenario using UI automation framework  
3\. Check the API endpoints of that flow  
4\. If the happy path works for a single user we add more loads continuously  
5\. 10 users at a time and up to fifty.  
6\. Then a functional tesitng is performed based on all the criterias.  
7\. A UI testing/ User Acceptance is performed.  
8\. A report is generated from Keyword driven, Arya, a/b testing tool,
blazemeter and a summary report is generated.  
7\. Based on the reports, if the outcome for each test are satisfactory then
the hours are entered in that ticket and Nahid deploys it to the Pilot server.

 **Load Testing**

1\. Make an entry in the easyredmine as scrum task to do.  
2\. Test the flow for a happy path scenario using UI automation framework.  
3\. Check the API endpoints of that flow.  
4\. If the happy path works for a single user we upload the jmx file in
Blazemeter and add more loads continuously in Blazemeter  
5\. Start with 10 users at a time and increase up to 50  
6\. We cover two cases-  
a) X number of users of Z concurrent user doing a task at random times within
test timeframe.  
b) Z number of users of Z concurrent user doing a task at a specific time
within test timeframe.  
7\. Based on the reports, if the outcome for each test are satisfactory then
the ticket is declared closed and Nahid deploys it to the Pilot server.  
  

  

## Reports

We generate different kind of reports for different sections. This helps us to
identify what caused the issue and this helps us to figure out the exact
solution also.

1\. Keyword Driven : It generates a report in csv format. The format shows
specific errors in a specific line. A sample report is attached to get an idea
how the reports are generated.  
2\. Arya: Currently we check logs to check all the findings if there occurs
any. We are planning to generate datewise reports for API endpoint statuses in
future.  
3\. A/B testing: Apache benchmarking tool generates a log file and we convert
it into a text format. A sample report is attached.  
4\. Blazemeter/Jmeter : Since we are using a free account Blazemeter lets us
to save the report for 15 days. We use that report and take that image and
attach that to our regression summary report. These are the sample report
images-

  

  

5\. Regression Testing: A brief summary report is generated and sent to Nahid
before any deployments. This helps to take decisions before any kind of
deployment. A sample report is attached

  

##  Attachments

  

250 250 250


